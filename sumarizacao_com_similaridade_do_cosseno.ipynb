{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMeYMarygmq3fDVAcj5Hi3I",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/oliveiraprg/text-summarization-with-python/blob/main/sumarizacao_com_similaridade_do_cosseno.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Sumarização com similaridade do cosseno"
      ],
      "metadata": {
        "id": "AV9hLaOrxp6P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preparação do ambiente"
      ],
      "metadata": {
        "id": "0UFdiz202669"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "1gpltgyoxbbY"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import nltk\n",
        "import string\n",
        "from nltk.cluster.util import cosine_distance\n",
        "import networkx as nx\n",
        "import numpy as np\n",
        "import spacy\n",
        "from spacy import displacy"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m spacy download pt_core_news_sm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X9seo1dutnDD",
        "outputId": "cddc396e-5fc1-477f-b937-d3191a9dacdd"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2022-10-06 23:57:09.537207: E tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pt-core-news-sm==3.4.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/pt_core_news_sm-3.4.0/pt_core_news_sm-3.4.0-py3-none-any.whl (13.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 13.0 MB 2.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: spacy<3.5.0,>=3.4.0 in /usr/local/lib/python3.7/dist-packages (from pt-core-news-sm==3.4.0) (3.4.1)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->pt-core-news-sm==3.4.0) (8.1.2)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->pt-core-news-sm==3.4.0) (1.0.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->pt-core-news-sm==3.4.0) (2.11.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->pt-core-news-sm==3.4.0) (57.4.0)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->pt-core-news-sm==3.4.0) (2.4.4)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->pt-core-news-sm==3.4.0) (3.3.0)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->pt-core-news-sm==3.4.0) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->pt-core-news-sm==3.4.0) (3.0.7)\n",
            "Requirement already satisfied: typing-extensions<4.2.0,>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->pt-core-news-sm==3.4.0) (4.1.1)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->pt-core-news-sm==3.4.0) (2.0.6)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->pt-core-news-sm==3.4.0) (0.6.2)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->pt-core-news-sm==3.4.0) (4.64.1)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->pt-core-news-sm==3.4.0) (2.23.0)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->pt-core-news-sm==3.4.0) (1.0.8)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->pt-core-news-sm==3.4.0) (0.10.1)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.10.0,>=1.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->pt-core-news-sm==3.4.0) (1.9.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->pt-core-news-sm==3.4.0) (21.3)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.9 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->pt-core-news-sm==3.4.0) (3.0.10)\n",
            "Requirement already satisfied: typer<0.5.0,>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->pt-core-news-sm==3.4.0) (0.4.2)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->pt-core-news-sm==3.4.0) (1.21.6)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from catalogue<2.1.0,>=2.0.6->spacy<3.5.0,>=3.4.0->pt-core-news-sm==3.4.0) (3.8.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->spacy<3.5.0,>=3.4.0->pt-core-news-sm==3.4.0) (3.0.9)\n",
            "Requirement already satisfied: smart-open<6.0.0,>=5.2.1 in /usr/local/lib/python3.7/dist-packages (from pathy>=0.3.5->spacy<3.5.0,>=3.4.0->pt-core-news-sm==3.4.0) (5.2.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->pt-core-news-sm==3.4.0) (2022.9.24)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->pt-core-news-sm==3.4.0) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->pt-core-news-sm==3.4.0) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->pt-core-news-sm==3.4.0) (1.24.3)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.7/dist-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.0->pt-core-news-sm==3.4.0) (0.0.2)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.7/dist-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.0->pt-core-news-sm==3.4.0) (0.7.8)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.7/dist-packages (from typer<0.5.0,>=0.3.0->spacy<3.5.0,>=3.4.0->pt-core-news-sm==3.4.0) (7.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy<3.5.0,>=3.4.0->pt-core-news-sm==3.4.0) (2.0.1)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('pt_core_news_sm')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dVxyX7972ZTU",
        "outputId": "163320b7-9430-45e7-f36a-3b70c0f3d3a9"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stop_words = nltk.corpus.stopwords.words('portuguese')"
      ],
      "metadata": {
        "id": "81zdSQzm2dMN"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def pre_processamento(texto):\n",
        "  texto_formatado = texto.lower()\n",
        "  tokens = []\n",
        "  for token in nltk.word_tokenize(texto_formatado):\n",
        "    tokens.append(token)\n",
        "  tokens = [palavra for palavra in tokens if palavra not in stop_words and palavra not in string.punctuation]\n",
        "  texto_formatado = ' '.join([str(elemento) for elemento in tokens if not elemento.isdigit()])\n",
        "  return texto_formatado"
      ],
      "metadata": {
        "id": "GajWXu1y2n4t"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "texto_original = \"\"\"A inteligência artificial é a inteligência similar à humana máquinas.\n",
        "                    Definem como o estudo de agente artificial com inteligência.\n",
        "                    Ciência e engenharia de produzir máquinas com inteligência.\n",
        "                    Resolver problemas e possuir inteligência. \n",
        "                    Relacionada ao comportamento inteligente. \n",
        "                    Construção de máquinas para raciocinar. \n",
        "                    Aprender com os erros e acertos. \n",
        "                    Inteligência artificial é raciocinar nas situações do cotidiano.\"\"\""
      ],
      "metadata": {
        "id": "ymCx702p3iE8"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "texto_original = re.sub(r'\\s+', ' ', texto_original)"
      ],
      "metadata": {
        "id": "XKzNAOe_3r3s"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Função para calcular similaridade entre sentenças"
      ],
      "metadata": {
        "id": "WWL6OzeM3rnk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentencas_originais = [sentenca for sentenca in nltk.sent_tokenize(texto_original)]\n",
        "sentencas_formatadas = [pre_processamento(sentencas_original) for sentencas_original in sentencas_originais]"
      ],
      "metadata": {
        "id": "ZB4uvslm4jcE"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calcula_similaridade_sentencas(primeira_sentenca, segunda_sentenca):\n",
        "  primeira_palavras = [palavra for palavra in nltk.word_tokenize(primeira_sentenca)]\n",
        "  segunda_palavras = [palavra for palavra in nltk.word_tokenize(segunda_sentenca)]\n",
        "  todas_palavras = list(set(primeira_palavras + segunda_palavras))\n",
        "  primeiro_vetor = [0] * len(todas_palavras)\n",
        "  segundo_vetor = [0] * len(todas_palavras)\n",
        "\n",
        "  for palavra in primeira_palavras:\n",
        "    primeiro_vetor[todas_palavras.index(palavra)] += 1 \n",
        "\n",
        "  for palavra in segunda_palavras:\n",
        "    segundo_vetor[todas_palavras.index(palavra)] += 1 \n",
        "\n",
        "  return 1 - cosine_distance(primeiro_vetor, segundo_vetor)"
      ],
      "metadata": {
        "id": "RTZYn4Bd8GVT"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Função para gerar a matriz de similaridade"
      ],
      "metadata": {
        "id": "Z_q4GVvTPRvL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calcula_matriz_similaridade(sentencas):\n",
        "  matriz_similaridade = np.zeros((len(sentencas), len(sentencas)))\n",
        "  for linha in range(len(sentencas)):\n",
        "    for coluna in range(len(sentencas)):\n",
        "      if linha == coluna:continue\n",
        "      matriz_similaridade[linha][coluna] = calcula_similaridade_sentencas(sentencas[linha], sentencas[coluna])\n",
        "  return matriz_similaridade"
      ],
      "metadata": {
        "id": "j3wsedeaPTOL"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Função para sumarizar"
      ],
      "metadata": {
        "id": "j15jJWLYfLg6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def sumarizar(texto):\n",
        "  sentencas_originais = [sentenca for sentenca in nltk.sent_tokenize(texto)]\n",
        "  sentencas_formatadas = [pre_processamento(sentenca_original) for sentenca_original in sentencas_originais]\n",
        "  quantidade_sentencas = int(len(sentencas_originais) / 2.5)\n",
        "  matriz_similaridade = calcula_matriz_similaridade(sentencas_formatadas)\n",
        "  grafo_similaridade = nx.from_numpy_array(matriz_similaridade)\n",
        "  notas = nx.pagerank(grafo_similaridade)\n",
        "  notas_ordenadas = sorted(((notas[indice_nota], nota) for indice_nota, nota in enumerate(sentencas_originais)), reverse=True)\n",
        "  melhores_sentencas = []\n",
        "  for sentenca in range(quantidade_sentencas):\n",
        "    melhores_sentencas.append(notas_ordenadas[sentenca][1])\n",
        "  return sentencas_originais, melhores_sentencas, notas_ordenadas"
      ],
      "metadata": {
        "id": "vkp26amQfNpp"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentencas_originais, melhores_sentencas, notas_ordenadas = sumarizar(texto_original)"
      ],
      "metadata": {
        "id": "T3BzGE7cp5ms"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentencas_originais, melhores_sentencas, notas_ordenadas "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kTKQfNVvrJEc",
        "outputId": "cb3d2f58-6124-4ce4-eb71-62f898b4956c"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(['A inteligência artificial é a inteligência similar à humana máquinas.',\n",
              "  'Definem como o estudo de agente artificial com inteligência.',\n",
              "  'Ciência e engenharia de produzir máquinas com inteligência.',\n",
              "  'Resolver problemas e possuir inteligência.',\n",
              "  'Relacionada ao comportamento inteligente.',\n",
              "  'Construção de máquinas para raciocinar.',\n",
              "  'Aprender com os erros e acertos.',\n",
              "  'Inteligência artificial é raciocinar nas situações do cotidiano.'],\n",
              " ['A inteligência artificial é a inteligência similar à humana máquinas.',\n",
              "  'Inteligência artificial é raciocinar nas situações do cotidiano.',\n",
              "  'Ciência e engenharia de produzir máquinas com inteligência.'],\n",
              " [(0.22820924040178003,\n",
              "   'A inteligência artificial é a inteligência similar à humana máquinas.'),\n",
              "  (0.1839190802151221,\n",
              "   'Inteligência artificial é raciocinar nas situações do cotidiano.'),\n",
              "  (0.1633191450783496,\n",
              "   'Ciência e engenharia de produzir máquinas com inteligência.'),\n",
              "  (0.1543717033327776,\n",
              "   'Definem como o estudo de agente artificial com inteligência.'),\n",
              "  (0.12639273963873288, 'Resolver problemas e possuir inteligência.'),\n",
              "  (0.09616903563964783, 'Construção de máquinas para raciocinar.'),\n",
              "  (0.023809527846794958, 'Relacionada ao comportamento inteligente.'),\n",
              "  (0.023809527846794958, 'Aprender com os erros e acertos.')])"
            ]
          },
          "metadata": {},
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Visualização do resumo"
      ],
      "metadata": {
        "id": "y4PyBFZprrz0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def visualiza_resumo(titulo, lista_sentencas, melhores_sentencas):\n",
        "  from IPython.core.display import HTML\n",
        "  texto = ''\n",
        "  resumo = ''\n",
        "  display(HTML(f'<h1>{titulo}</h1>'))\n",
        "  display(HTML(f'<h2>Texto com resumo destacado - {titulo}</h2>'))\n",
        "  for sentenca in lista_sentencas:\n",
        "    if sentenca in melhores_sentencas:\n",
        "      texto += str(sentenca).replace(sentenca, f'<mark>{sentenca} </mark>')\n",
        "      resumo += str(sentenca).replace(sentenca, f'{sentenca} ')\n",
        "    else:\n",
        "      texto += f'{sentenca} '\n",
        "  display(HTML(f'{texto}'))\n",
        "  display(HTML(f'<h2>Resumo de - {titulo}</h2>'))\n",
        "  display(HTML(f'{resumo}'))"
      ],
      "metadata": {
        "id": "nIlR7i6mruxP"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Extração de texto da internet"
      ],
      "metadata": {
        "id": "riTbVEmYlwlK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install goose3"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UAL1J3Arl9XD",
        "outputId": "bbc803d9-fa15-420c-cad1-b8e7914fc5ad"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: goose3 in /usr/local/lib/python3.7/dist-packages (3.1.12)\n",
            "Requirement already satisfied: langdetect in /usr/local/lib/python3.7/dist-packages (from goose3) (1.0.9)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.7/dist-packages (from goose3) (4.6.3)\n",
            "Requirement already satisfied: pyahocorasick in /usr/local/lib/python3.7/dist-packages (from goose3) (1.4.4)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.7/dist-packages (from goose3) (7.1.2)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.7/dist-packages (from goose3) (4.9.1)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.7/dist-packages (from goose3) (2.8.2)\n",
            "Requirement already satisfied: cssselect in /usr/local/lib/python3.7/dist-packages (from goose3) (1.1.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from goose3) (2.23.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from langdetect->goose3) (1.15.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->goose3) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->goose3) (2022.9.24)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->goose3) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->goose3) (3.0.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from goose3 import Goose"
      ],
      "metadata": {
        "id": "BKQCg12kmhe6"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "g = Goose()\n",
        "url = 'https://iaexpert.academy/2020/11/09/ia-preve-resultado-das-eleicoes-americanas/'\n",
        "artigo = g.extract(url)"
      ],
      "metadata": {
        "id": "T_-DCGfqmkRT"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentencas_originais, melhores_sentencas, notas_ordenadas = sumarizar(artigo.cleaned_text)"
      ],
      "metadata": {
        "id": "6UUcL158m41S"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "visualiza_resumo(artigo.title, sentencas_originais, melhores_sentencas)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 426
        },
        "id": "aOf40RvPnorK",
        "outputId": "21ece174-4407-48d1-b5ec-1f13237659df"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<h1>IA prevê resultado das eleições americanas – IA Expert Academy</h1>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<h2>Texto com resumo destacado - IA prevê resultado das eleições americanas – IA Expert Academy</h2>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Nas eleições presidenciais americanas de 2016, a maioria das predições apontavam para a vitória de Hillary Clinton. Entretanto, a história nos mostrou o resultado oposto, e Donald Trump foi o presidente nos últimos 4 anos. Desta vez, os estatísticos reexaminaram seus modelos, para aumentar o grau de confiabilidade nos seus resultados. Nesta tentativa de otimização das predições, a inteligência artificial certamente não ficou de fora. <mark>O modelo desenvolvido pelo Dr. Hernan Makse, físico estatístico da Universidade da Cidade de Nova York, baseou suas predições em uma rede neural treinada para processar os sentimentos expressos nas redes sociais. </mark>O algoritmo fez a análise de cerca de 1 bilhão de tweets para chegar a uma estimativa dos resultados do pleito. <mark>No dia da eleição, 3 de novembro, o modelo estava indicando a vitória de Joe Biden. </mark><mark>O Dr. Makse disse que seu trabalho começou já na eleição de 2016, e foi testado novamente nas eleições na Argentina ano passado. </mark><mark>Desta vez, o modelo está treinando com cerca de 5 vezes mais dados que nas eleições americanas anteriores. </mark>O trabalho não depende apenas da coleta dos dados, mas também de um tratamento estatístico adequado para levar em consideração duas variáveis externas: o viés de amostragem e a taxa de comparecimento. O primeiro fator se refere ao fato de que as redes sociais não necessariamente representam a população americana. A participação em redes sociais costuma ser maior nas cidades grandes, que de fato têm preferência por um dos candidatos, e o modelo deve ser corrigido para levar em consideração também a opinião das pessoas que não são ativas neste ambiente virtual. O segundo fator se deve à não-obrigatoriedade de votação nos Estados Unidos, ou seja, por mais que uma pessoa tenha sua preferência, pode ser que ela não compareça aos locais de votação para efetivá-la. <mark>Segundo o Dr. Makse, integrar estas duas variáveis em seu modelo é a parte mais importante do trabalho. </mark>Ele acredita ser esta uma das razões para que as estimativas da última eleição, baseadas em métodos tradicionais de coleta de informação, terem falhado. Sua equipe acompanhou as tendências apresentadas nas últimas eleições na Europa, e os modelos estão se revelando cada vez melhores. <mark>Quando seu modelo foi usado para predizer os resultados da eleição corrente usando dados brutos, Joe Biden apareceu como vencedor com larga vantagem. </mark>Após aplicar os mecanismos de correção para os dois vieses identificados, a vantagem diminuiu, mas Biden ainda é indicado como favorito. <mark>Parece que, desta vez, os algoritmos estão de fato contribuindo para que as predições sejam mais precisas. </mark>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<h2>Resumo de - IA prevê resultado das eleições americanas – IA Expert Academy</h2>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "O modelo desenvolvido pelo Dr. Hernan Makse, físico estatístico da Universidade da Cidade de Nova York, baseou suas predições em uma rede neural treinada para processar os sentimentos expressos nas redes sociais. No dia da eleição, 3 de novembro, o modelo estava indicando a vitória de Joe Biden. O Dr. Makse disse que seu trabalho começou já na eleição de 2016, e foi testado novamente nas eleições na Argentina ano passado. Desta vez, o modelo está treinando com cerca de 5 vezes mais dados que nas eleições americanas anteriores. Segundo o Dr. Makse, integrar estas duas variáveis em seu modelo é a parte mais importante do trabalho. Quando seu modelo foi usado para predizer os resultados da eleição corrente usando dados brutos, Joe Biden apareceu como vencedor com larga vantagem. Parece que, desta vez, os algoritmos estão de fato contribuindo para que as predições sejam mais precisas. "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Lematização com algoritimo de Luhn"
      ],
      "metadata": {
        "id": "dfZdsGoSrrGa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pln = spacy.load('pt_core_news_sm')"
      ],
      "metadata": {
        "id": "1Rc9v-3QrxrZ"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def pre_processamento_lematizacao(texto):\n",
        "  texto = texto.lower()\n",
        "  texto = re.sub(\"\\s+\", ' ', texto)\n",
        "  documento = pln(texto)\n",
        "  tokens = []\n",
        "  for token in documento:\n",
        "    tokens.append(token.lemma_)\n",
        "  tokens = [palavra for palavra in tokens if palavra not in stop_words and palavra not in string.punctuation]\n",
        "  texto_formatado = ' '.join([str(elemento) for elemento in tokens if not elemento.isdigit()])\n",
        "  return texto_formatado"
      ],
      "metadata": {
        "id": "VoU42TdQr77R"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sumarizar_lematizacao(texto):\n",
        "  sentencas_originais = [sentenca for sentenca in nltk.sent_tokenize(texto)]\n",
        "  sentencas_formatadas = [pre_processamento_lematizacao(sentenca_original) for sentenca_original in sentencas_originais]\n",
        "  palavras = [palavra.lower() for sentenca in sentencas_formatadas for palavra in nltk.word_tokenize(sentenca)]\n",
        "  quantidade_sentencas = int(len(sentencas_originais) / 2.5)\n",
        "  matriz_similaridade = calcula_matriz_similaridade(sentencas_formatadas)\n",
        "  grafo_similaridade = nx.from_numpy_array(matriz_similaridade)\n",
        "  notas = nx.pagerank(grafo_similaridade)\n",
        "  notas_ordenadas = sorted(((notas[indice_nota], nota) for indice_nota, nota in enumerate(sentencas_originais)), reverse=True)\n",
        "  melhores_sentencas = []\n",
        "  for sentenca in range(quantidade_sentencas):\n",
        "    melhores_sentencas.append(notas_ordenadas[sentenca][1])\n",
        "  return sentencas_originais, melhores_sentencas, notas_ordenadas"
      ],
      "metadata": {
        "id": "OzI7mCScsfZp"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentencas_originais_lematizacao, melhores_sentencas_lematizacao, notas_ordenadas_lematizacao = sumarizar_lematizacao(artigo.cleaned_text)"
      ],
      "metadata": {
        "id": "tRrYVW9EuU2Y"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "visualiza_resumo(artigo.title, sentencas_originais_lematizacao, melhores_sentencas_lematizacao)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 443
        },
        "id": "7O_BM5sGu5nk",
        "outputId": "a33f707e-efde-4e1b-c76f-1c80045f571d"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<h1>IA prevê resultado das eleições americanas – IA Expert Academy</h1>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<h2>Texto com resumo destacado - IA prevê resultado das eleições americanas – IA Expert Academy</h2>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<mark>Nas eleições presidenciais americanas de 2016, a maioria das predições apontavam para a vitória de Hillary Clinton. </mark>Entretanto, a história nos mostrou o resultado oposto, e Donald Trump foi o presidente nos últimos 4 anos. Desta vez, os estatísticos reexaminaram seus modelos, para aumentar o grau de confiabilidade nos seus resultados. Nesta tentativa de otimização das predições, a inteligência artificial certamente não ficou de fora. <mark>O modelo desenvolvido pelo Dr. Hernan Makse, físico estatístico da Universidade da Cidade de Nova York, baseou suas predições em uma rede neural treinada para processar os sentimentos expressos nas redes sociais. </mark>O algoritmo fez a análise de cerca de 1 bilhão de tweets para chegar a uma estimativa dos resultados do pleito. <mark>No dia da eleição, 3 de novembro, o modelo estava indicando a vitória de Joe Biden. </mark><mark>O Dr. Makse disse que seu trabalho começou já na eleição de 2016, e foi testado novamente nas eleições na Argentina ano passado. </mark>Desta vez, o modelo está treinando com cerca de 5 vezes mais dados que nas eleições americanas anteriores. O trabalho não depende apenas da coleta dos dados, mas também de um tratamento estatístico adequado para levar em consideração duas variáveis externas: o viés de amostragem e a taxa de comparecimento. O primeiro fator se refere ao fato de que as redes sociais não necessariamente representam a população americana. <mark>A participação em redes sociais costuma ser maior nas cidades grandes, que de fato têm preferência por um dos candidatos, e o modelo deve ser corrigido para levar em consideração também a opinião das pessoas que não são ativas neste ambiente virtual. </mark>O segundo fator se deve à não-obrigatoriedade de votação nos Estados Unidos, ou seja, por mais que uma pessoa tenha sua preferência, pode ser que ela não compareça aos locais de votação para efetivá-la. Segundo o Dr. Makse, integrar estas duas variáveis em seu modelo é a parte mais importante do trabalho. <mark>Ele acredita ser esta uma das razões para que as estimativas da última eleição, baseadas em métodos tradicionais de coleta de informação, terem falhado. </mark><mark>Sua equipe acompanhou as tendências apresentadas nas últimas eleições na Europa, e os modelos estão se revelando cada vez melhores. </mark>Quando seu modelo foi usado para predizer os resultados da eleição corrente usando dados brutos, Joe Biden apareceu como vencedor com larga vantagem. Após aplicar os mecanismos de correção para os dois vieses identificados, a vantagem diminuiu, mas Biden ainda é indicado como favorito. Parece que, desta vez, os algoritmos estão de fato contribuindo para que as predições sejam mais precisas. "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<h2>Resumo de - IA prevê resultado das eleições americanas – IA Expert Academy</h2>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Nas eleições presidenciais americanas de 2016, a maioria das predições apontavam para a vitória de Hillary Clinton. O modelo desenvolvido pelo Dr. Hernan Makse, físico estatístico da Universidade da Cidade de Nova York, baseou suas predições em uma rede neural treinada para processar os sentimentos expressos nas redes sociais. No dia da eleição, 3 de novembro, o modelo estava indicando a vitória de Joe Biden. O Dr. Makse disse que seu trabalho começou já na eleição de 2016, e foi testado novamente nas eleições na Argentina ano passado. A participação em redes sociais costuma ser maior nas cidades grandes, que de fato têm preferência por um dos candidatos, e o modelo deve ser corrigido para levar em consideração também a opinião das pessoas que não são ativas neste ambiente virtual. Ele acredita ser esta uma das razões para que as estimativas da última eleição, baseadas em métodos tradicionais de coleta de informação, terem falhado. Sua equipe acompanhou as tendências apresentadas nas últimas eleições na Europa, e os modelos estão se revelando cada vez melhores. "
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}